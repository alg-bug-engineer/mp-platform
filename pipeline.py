import requests
import json
import os
import re
import base64
import time
import uuid
from pathlib import Path
from typing import Dict, Iterable, Tuple, Optional

from bs4 import BeautifulSoup
from PIL import Image
from bs4 import BeautifulSoup, NavigableString
import os
import re  # æ–°å¢æ­£åˆ™æ¨¡å—
from playwright.sync_api import sync_playwright

try:
    import markdown as simple_markdown
except ImportError:
    simple_markdown = None

class WeChatDraftHelper:
    def __init__(self, appid, appsecret):
        self.appid = appid
        self.appsecret = appsecret
        self.token = None
        self.token_expires_at = 0
        
        base_dir = Path(__file__).resolve().parent
        self.asset_dir = base_dir / "imgs"
        self.asset_dir.mkdir(exist_ok=True)
            
        # é™åˆ¶é…ç½®
        self.max_article_img_size = 1 * 1024 * 1024  # æ­£æ–‡å›¾ç‰‡é™åˆ¶ 1MB
        self.max_cover_img_size = 9 * 1024 * 1024    # å°é¢å›¾ç‰‡é™åˆ¶ 10MB (ç•™ç‚¹ä½™é‡)

    def get_access_token(self):
        """è·å–æˆ–åˆ·æ–° Access Token"""
        if self.token and time.time() < self.token_expires_at:
            return self.token
            
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.appid}&secret={self.appsecret}"
        resp = requests.get(url)
        data = resp.json()
        
        if 'access_token' in data:
            self.token = data['access_token']
            # æå‰ 5 åˆ†é’Ÿè¿‡æœŸï¼Œé˜²æ­¢ä¸´ç•Œç‚¹é—®é¢˜
            self.token_expires_at = time.time() + data['expires_in'] - 300
            print(f"âœ… è·å– Access Token æˆåŠŸ")
            return self.token
        else:
            raise Exception(f"è·å– Token å¤±è´¥: {data}")

    def _compress_image(self, local_path, max_size):
        """é€šç”¨å›¾ç‰‡å‹ç¼©é€»è¾‘"""
        if not os.path.exists(local_path):
            return None
            
        file_size = os.path.getsize(local_path)
        if file_size <= max_size:
            return local_path

        print(f"   âš ï¸ å›¾ç‰‡({file_size/1024:.0f}KB)è¶…é™ï¼Œæ­£åœ¨å‹ç¼©...")
        try:
            img = Image.open(local_path)
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            quality = 85
            scale = 0.9
            target_path = local_path # è¦†ç›–åŸæ–‡ä»¶
            
            while True:
                img.save(target_path, format='JPEG', quality=quality)
                if os.path.getsize(target_path) <= max_size:
                    break
                
                if quality > 30:
                    quality -= 10
                else:
                    width, height = img.size
                    img = img.resize((int(width * scale), int(height * scale)), Image.LANCZOS)
        except Exception as e:
            print(f"   âŒ å‹ç¼©å¼‚å¸¸: {e}")
            
        return local_path

    def upload_cover_image(self, file_path):
        """
        æ­¥éª¤Aï¼šä¸Šä¼ å°é¢å›¾ç‰‡ (æ°¸ä¹…ç´ æ)
        è¿”å›: media_id
        """
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/material/add_material?access_token={token}&type=image"
        
        # 1. å‹ç¼©å°é¢åˆ° 10MB ä»¥å†…
        processed_path = self._compress_image(file_path, self.max_cover_img_size)
        
        try:
            filename = os.path.basename(processed_path)
            with open(processed_path, 'rb') as f:
                # å¿…é¡»æŒ‡å®š filenameï¼Œå¦åˆ™å¾®ä¿¡å¯èƒ½æŠ¥é”™
                files = {'media': (filename, f, 'image/jpeg')}
                resp = requests.post(url, files=files)
                
            res = resp.json()
            if 'media_id' in res:
                print(f"âœ… å°é¢ä¸Šä¼ æˆåŠŸ: {res['media_id']}")
                return res['media_id']
            else:
                print(f"âŒ å°é¢ä¸Šä¼ å¤±è´¥: {res}")
                return None
        except Exception as e:
            print(f"âŒ ä¸Šä¼ è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def _upload_article_img(self, local_path):
        """å†…éƒ¨æ–¹æ³•ï¼šä¸Šä¼ æ­£æ–‡æ’å›¾ (è¿”å› URL)"""
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/media/uploadimg?access_token={token}"
        
        # 1. å‹ç¼©åˆ° 1MB ä»¥å†…
        processed_path = self._compress_image(local_path, self.max_article_img_size)
        
        try:
            filename = os.path.basename(processed_path)
            with open(processed_path, 'rb') as f:
                files = {'media': (filename, f, 'image/jpeg')}
                resp = requests.post(url, files=files)
            
            res = resp.json()
            if 'url' in res:
                return res['url']
            else:
                print(f"   âŒ æ­£æ–‡å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {res}")
                return None
        except Exception as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def _save_base64_to_local(self, base64_str, prefix="temp_b64"):
        """Base64 -> æœ¬åœ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            pattern = r'data:image/(\w+);base64,(.+)'
            match = re.search(pattern, base64_str, re.DOTALL)
            if not match: return None
            
            ext = match.group(1).replace('jpeg', 'jpg')
            data = base64.b64decode(match.group(2))
            
            filename = f"{prefix}_{uuid.uuid4().hex}.{ext}"
            path = os.path.join(self.asset_dir, filename)
            with open(path, 'wb') as f:
                f.write(data)
            return path
        except Exception:
            return None

    def _save_url_to_local(self, url):
        """HTTP URL -> æœ¬åœ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            resp = requests.get(url, headers=headers, timeout=15)
            if resp.status_code != 200: return None
            
            filename = f"temp_net_{uuid.uuid4().hex}.jpg"
            path = os.path.join(self.asset_dir, filename)
            with open(path, 'wb') as f:
                f.write(resp.content)
            return path
        except Exception:
            return None

    def save_image_from_base64(self, base64_str, prefix="img"):
        """å¯¹å¤–æš´éœ²çš„ Base64 å­˜å‚¨æ¥å£"""
        return self._save_base64_to_local(base64_str, prefix=prefix)

    def save_image_from_url(self, url):
        """å¯¹å¤–æš´éœ²çš„ URL å›¾ç‰‡ä¸‹è½½æ¥å£"""
        return self._save_url_to_local(url)

    def process_html_content(self, html_content):
        """
        æ­¥éª¤Bï¼šå¤„ç† HTML æ­£æ–‡
        å°† Base64 å’Œå¤–é“¾æ›¿æ¢ä¸ºå¾®ä¿¡ URL
        """
        if not html_content:
            return "", {}
        
        print("ğŸ”„ å¼€å§‹æ¸…æ´—æ­£æ–‡ HTML...")
        soup = BeautifulSoup(html_content, 'html.parser')
        imgs = soup.find_all('img')
        
        replacements: Dict[str, str] = {}
        count = 0
        for img in imgs:
            src = img.get('src')
            if not src: continue
            # è·³è¿‡å·²æœ‰å¾®ä¿¡é“¾æ¥
            if 'mmbiz.qpic.cn' in src: continue
            
            local_path = None
            if src.startswith('data:image'):
                local_path = self._save_base64_to_local(src)
            elif src.startswith('http'):
                local_path = self._save_url_to_local(src)
            
            if local_path:
                # ä¸Šä¼ å¹¶è·å–å¾®ä¿¡ URL
                wechat_url = self._upload_article_img(local_path)
                if wechat_url:
                    replacements[src] = wechat_url
                    img['src'] = wechat_url
                    # æ¸…ç†æ‚é¡¹å±æ€§
                    for attr in ['data-src', 'style', 'width', 'height']:
                        if img.get(attr): del img[attr]
                    count += 1
        
        print(f"âœ… æ­£æ–‡æ¸…æ´—å®Œæˆï¼Œæ›¿æ¢äº† {count} å¼ å›¾ç‰‡ã€‚")
        return str(soup), replacements

    def submit_draft(self, articles_data):
        """
        æ­¥éª¤Cï¼šæäº¤åˆ°è‰ç¨¿ç®±
        :param articles_data: åŒ…å«æ–‡ç« å­—å…¸çš„åˆ—è¡¨ [{}, {}]
        """
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={token}"
        
        payload = {
            "articles": articles_data
        }
        
        # ensure_ascii=False ç¡®ä¿ä¸­æ–‡æ­£å¸¸æ˜¾ç¤ºï¼Œä¸ä¼šè½¬ä¹‰
        json_data = json.dumps(payload, ensure_ascii=False).encode('utf-8')
        headers = {'Content-Type': 'application/json'}
        
        try:
            resp = requests.post(url, data=json_data, headers=headers)
            res = resp.json()
            if 'media_id' in res:
                print(f"ğŸ‰ è‰ç¨¿å‘å¸ƒæˆåŠŸï¼Media ID: {res['media_id']}")
                return res['media_id']
            else:
                print(f"âŒ è‰ç¨¿å‘å¸ƒå¤±è´¥: {res}")
                return None
        except Exception as e:
            print(f"âŒ æäº¤è¯·æ±‚å¼‚å¸¸: {e}")
            return None

def _markdown_to_html(md_text: str) -> str:
    if not md_text:
        return ""
    if simple_markdown is None:
        raise ImportError("æœªæ£€æµ‹åˆ° markdown åº“ï¼Œè¯·å…ˆæ‰§è¡Œ pip install markdown")
    return simple_markdown.markdown(
        md_text,
        extensions=[
            "extra",
            "fenced_code",
            "tables"
        ]
    )


def _extract_first_base64(markdown_text: str) -> Optional[str]:
    if not markdown_text:
        return None
    pattern = re.compile(r'(data:image/[a-zA-Z0-9.+-]+;base64,[^)]+)', re.MULTILINE)
    match = pattern.search(markdown_text)
    return match.group(1) if match else None


def _extract_first_image_url(markdown_text: str) -> Optional[str]:
    """ä» Markdown ä¸­æå–ç¬¬ä¸€ä¸ªå›¾ç‰‡ URLï¼ˆé base64ï¼‰"""
    if not markdown_text:
        return None
    # åŒ¹é… Markdown å›¾ç‰‡è¯­æ³• ![alt](url) æˆ– HTML <img src="url">
    md_pattern = re.compile(r'!\[[^\]]*\]\((https?://[^)]+)\)', re.MULTILINE)
    match = md_pattern.search(markdown_text)
    if match:
        return match.group(1)
    # å°è¯•åŒ¹é… HTML img æ ‡ç­¾
    html_pattern = re.compile(r'<img[^>]+src=["\']?(https?://[^"\'>\s]+)["\']?', re.IGNORECASE)
    match = html_pattern.search(markdown_text)
    return match.group(1) if match else None


def _extract_title(markdown_text: str, fallback: str) -> str:
    if markdown_text:
        heading_match = re.search(r'^\s*#\s+(.+)$', markdown_text, re.MULTILINE)
        if heading_match:
            return heading_match.group(1).strip()
    return fallback


def _build_digest(html_content: str, limit: int = 120) -> str:
    if not html_content:
        return ""
    soup = BeautifulSoup(html_content, 'html.parser')
    text = soup.get_text(separator=' ', strip=True)
    return text[:limit]


def _replace_markdown_sources(markdown_text: str, replacements: Dict[str, str]) -> str:
    if not markdown_text or not replacements:
        return markdown_text
    updated = markdown_text
    for old_src, new_src in replacements.items():
        updated = updated.replace(old_src, new_src)
    return updated


def _save_history_markdown(history_dir: Path, file_name: str, content: str):
    history_dir.mkdir(exist_ok=True)
    history_path = history_dir / file_name
    with history_path.open("w", encoding="utf-8") as f:
        f.write(content)
    print(f"ğŸ“ å·²ä¿å­˜æ¸…æ´—åçš„ Markdown åˆ° {history_path}")
    return history_path


def _save_formatted_html(html_dir: Path, file_name: str, content: str):
    html_dir.mkdir(exist_ok=True)
    html_path = html_dir / file_name
    with html_path.open("w", encoding="utf-8") as f:
        f.write(content)
    print(f"ğŸ§¾ å·²ä¿å­˜æ’ç‰ˆ HTML åˆ° {html_path}")


def _iter_markdown_files(posts_dir: Path) -> Iterable[Tuple[Path, str]]:
    if not posts_dir.exists():
        raise FileNotFoundError(f"posts ç›®å½•ä¸å­˜åœ¨: {posts_dir}")
    for md_file in sorted(posts_dir.glob("*.md")):
        with md_file.open("r", encoding="utf-8") as f:
            yield md_file, f.read()


def format_markdown(markdown_content):
    with sync_playwright() as p:
        print("[ç³»ç»Ÿ] æ­£åœ¨å¯åŠ¨æ— å¤´æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(permissions=["clipboard-read", "clipboard-write"])
        page = context.new_page()

        print("[ç³»ç»Ÿ] æ­£åœ¨æ‰“å¼€ https://www.md2wechat.com/ ...")
        page.goto("https://www.md2wechat.com/")

        # --- å‰ªè´´æ¿ Mock (ä¿æŒä¸å˜) ---
        page.evaluate("""
            window._capturedContent = '';
            navigator.clipboard.writeText = async (text) => {
                window._capturedContent = text;
                return Promise.resolve();
            };
            navigator.clipboard.write = async (data) => {
                let items = [];
                if (Array.isArray(data)) items = data;
                else if (data && typeof data[Symbol.iterator] === 'function') items = Array.from(data);
                else if (data) items = [data];

                for (const item of items) {
                    try {
                        if (item && typeof item.getType === 'function') {
                            if (item.types.includes('text/html')) {
                                const blob = await item.getType('text/html');
                                window._capturedContent = await blob.text();
                            } else if (item.types.includes('text/plain')) {
                                const blob = await item.getType('text/plain');
                                window._capturedContent = await blob.text();
                            }
                        }
                    } catch (err) { console.error(err); }
                }
                return Promise.resolve();
            };
        """)
        print("[ç³»ç»Ÿ] å‰ªè´´æ¿ Mock æ‹¦æˆªå™¨å·²æ³¨å…¥ã€‚")

        # å¡«å…¥å†…å®¹
        print("[ç³»ç»Ÿ] æ­£åœ¨å¡«å…¥ Markdown å†…å®¹...")
        textarea_selector = 'textarea[placeholder="åœ¨æ­¤å¤„è¾“å…¥ Markdown å†…å®¹..."]'
        page.wait_for_selector(textarea_selector)
        page.fill(textarea_selector, markdown_content)

        print("[ç³»ç»Ÿ] ç­‰å¾… 5 ç§’è®©é¡µé¢è¿›è¡Œæ¸²æŸ“...")
        page.wait_for_timeout(5000)

        print("[ç³»ç»Ÿ] ç‚¹å‡»â€œå¤åˆ¶â€æŒ‰é’®...")
        button_selector = 'button:has-text("å¤åˆ¶")'
        page.wait_for_selector(button_selector)
        page.click(button_selector)

        page.wait_for_timeout(1000) 
        copied_content = page.evaluate("window._capturedContent")

        if not copied_content:
            print("[è­¦å‘Š] æœªæ•è·åˆ°å†…å®¹ï¼")
        else:
            print(f"[ç³»ç»Ÿ] æˆåŠŸæ•è·å†…å®¹ï¼Œé•¿åº¦: {len(copied_content)}")
            
            # --- ç»Ÿä¸€åå¤„ç†é€»è¾‘ (ä½¿ç”¨ BeautifulSoup) ---
            print("[ç³»ç»Ÿ] æ­£åœ¨æ‰§è¡Œ HTML æ·±åº¦åå¤„ç†...")
            try:
                soup = BeautifulSoup(copied_content, 'html.parser')

                # 1. åˆ é™¤åŒ…å« "å›¾ï¼š" çš„ em æ ‡ç­¾
                for em in soup.find_all('em'):
                    if 'å›¾ï¼š' in em.get_text():
                        em.decompose()
                        print("[ç³»ç»Ÿ] å·²åˆ é™¤ä¸€ä¸ªå›¾ç‰‡è¯´æ˜æ ‡ç­¾")

                # 2. ç»™æ‰€æœ‰ <p> æ ‡ç­¾å¢åŠ é¦–è¡Œç¼©è¿›
                # ã€é‡è¦ä¿®æ”¹ã€‘æ’é™¤æ‰ li å†…éƒ¨çš„ p æ ‡ç­¾ï¼Œé˜²æ­¢åˆ—è¡¨æ–‡å­—ç›¸å¯¹äºç¬¦å·å‘ç”Ÿé”™ä½
                for p in soup.find_all('p'):
                    if p.parent and p.parent.name == 'li':
                        continue  # è·³è¿‡åˆ—è¡¨ä¸­çš„æ®µè½
                    
                    current_style = p.get('style', '')
                    p['style'] = f"{current_style}; text-indent: 2em;"

                # =====================================================
                # 3. åˆ—è¡¨åµŒå¥—ç¼©è¿›å¤„ç† & å¼ºåˆ¶å¢åŠ æ¢è¡Œ
                # =====================================================
                # é€»è¾‘ï¼šæŸ¥æ‰¾æ‰€æœ‰ä½œä¸º li ç›´æ¥å­å…ƒç´ çš„ ul æˆ– ol (å³åµŒå¥—å­åˆ—è¡¨)
                nested_lists = [tag for tag in soup.find_all(['ul', 'ol']) if tag.parent and tag.parent.name == 'li']
                
                if nested_lists:
                    print(f"[ç³»ç»Ÿ] æ£€æµ‹åˆ° {len(nested_lists)} ä¸ªåµŒå¥—å­åˆ—è¡¨ï¼Œæ­£åœ¨æ‰§è¡Œâ€œå¼ºåˆ¶æ¢è¡Œä¸ç¼©è¿›â€å¤„ç†...")
                    for lst in nested_lists:
                        # A. æ ·å¼ç¼©è¿› (å¢åŠ å·¦å¡«å……ï¼Œä½“ç°å±‚çº§)
                        # ä¿®æ”¹è¯´æ˜ï¼šä½¿ç”¨ padding-left æ›¿ä»£å•çº¯çš„ marginï¼Œè¿™æ›´èƒ½ä¿è¯ç¬¦å·(bullet)è·Ÿç€å†…å®¹ä¸€èµ·ç§»åŠ¨ã€‚
                        # padding-left: 40px æ˜¯æµè§ˆå™¨çš„é»˜è®¤å€¼ï¼Œè¿™é‡Œæˆ‘ä»¬è®¾ç½®ä¸º 2em ç¡®ä¿ç¼©è¿›å¯è§ã€‚
                        current_style = lst.get('style', '')
                        lst['style'] = f"{current_style}; padding-left: 2em; list-style-position: outside;"
                        
                        # C. ã€éœ€æ±‚å®ç°ã€‘å­åˆ—è¡¨å†…çš„æ¯ä¸€ä¸ª li å‰æ’å…¥ br
                        child_lis = lst.find_all('li', recursive=False)
                        for child_li in child_lis:
                            br_tag = soup.new_tag('br')
                            child_li.insert_before(br_tag)

                # 4. åˆ—è¡¨ç¬¦å·ä¸å†…å®¹é—´è·å¤„ç†
                for li in soup.find_all('li'):
                    if li.contents and isinstance(li.contents[0], NavigableString):
                        text_node = li.contents[0]
                        match = re.match(r'^(\s*(?:\d+\.|[â€¢\-Â·]))(\s+)', text_node)
                        if match:
                            marker = match.group(1)
                            space = match.group(2)
                            new_text = marker + space + '\u3000' + text_node[match.end():]
                            text_node.replace_with(new_text)

                # ç”Ÿæˆæœ€ç»ˆ HTML
                final_html = str(soup)
            except Exception as e:
                print(f"[é”™è¯¯] HTML åå¤„ç†å¤±è´¥: {e}")
                # å¦‚æœ BS4 å¤„ç†å¤±è´¥ï¼Œä¿å­˜åŸå§‹å†…å®¹ä½œä¸ºå¤‡ä»½
                final_html = None

        browser.close()
        return final_html


def publish_posts_to_wechat(helper: WeChatDraftHelper, posts_dir: Path):
    history_dir = posts_dir.parent / "history"
    html_history_dir = posts_dir.parent / "history_html"
    for md_path, md_body in _iter_markdown_files(posts_dir):
        article_name = md_path.stem
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ç« ã€Š{article_name}ã€‹...")

        cover_path = None

        # ä¼˜å…ˆå°è¯•æå– Base64 å°é¢
        cover_base64 = _extract_first_base64(md_body)
        if cover_base64:
            print(f"   ğŸ–¼ï¸ æ£€æµ‹åˆ° Base64 å°é¢å›¾ç‰‡")
            cover_path = helper.save_image_from_base64(cover_base64, prefix=f"{article_name}_cover")
        else:
            # å°è¯•æå– URL å°é¢
            cover_url = _extract_first_image_url(md_body)
            if cover_url:
                print(f"   ğŸŒ æ£€æµ‹åˆ° URL å°é¢å›¾ç‰‡: {cover_url[:60]}...")
                cover_path = helper.save_image_from_url(cover_url)

        if not cover_path:
            print(f"âš ï¸ æœªåœ¨ {md_path.name} ä¸­æ‰¾åˆ°æœ‰æ•ˆå°é¢å›¾ç‰‡ï¼Œå·²è·³è¿‡ã€‚")
            continue

        thumb_media_id = helper.upload_cover_image(cover_path)
        if not thumb_media_id:
            print(f"âš ï¸ å°é¢ä¸Šä¼ å¤±è´¥ï¼Œè·³è¿‡ {md_path.name}")
            continue

        try:
            html_content = _markdown_to_html(md_body)
        except ImportError as err:
            print(f"âŒ Markdown è½¬ HTML å¤±è´¥: {err}")
            break

        clean_content, replacement_map = helper.process_html_content(html_content)
        if not clean_content:
            clean_content = html_content

        cleaned_markdown = _replace_markdown_sources(md_body, replacement_map)
        _save_history_markdown(history_dir, md_path.name, cleaned_markdown)

        formatted_html = format_markdown(cleaned_markdown)
        if formatted_html:
            print("âœ¨ ä½¿ç”¨ Format æ’ç‰ˆ HTML")
            html_file_name = md_path.with_suffix(".html").name
            _save_formatted_html(html_history_dir, html_file_name, formatted_html)
        else:
            print("â„¹ï¸ Format æ’ç‰ˆå¤±è´¥ï¼Œæ”¹ç”¨æ™®é€š HTML")
        final_content = formatted_html or clean_content
        print(f"ğŸ“Œ å½“å‰æ­£æ–‡æ¥æº: {'Format' if formatted_html else 'æ™®é€š HTML'}")
        article_title = _extract_title(md_body, fallback=article_name)
        digest = _build_digest(final_content)

        article_payload = {
            "article_type": "news",
            "title": article_title,
            "author": "èŠå£«AIåƒé±¼",
            "digest": "",
            "content": final_content,
            "content_source_url": "",
            "thumb_media_id": thumb_media_id,
            "need_open_comment": 1,
            "only_fans_can_comment": 0
        }
        
        print(f"ğŸš€ æ­£åœ¨æäº¤ã€Š{article_title}ã€‹åˆ°è‰ç¨¿ç®±...")
        helper.submit_draft([article_payload])


def _load_app_config():
    app_id = "wx7643d18e996418be"
    app_secret = "6bb1e622f572f1d8008a5a03ad421030"
    if not app_id or not app_secret:
        raise EnvironmentError("è¯·å…ˆé€šè¿‡ç¯å¢ƒå˜é‡ WECHAT_APP_ID / WECHAT_APP_SECRET é…ç½®å…¬ä¼—å·å‡­è¯ã€‚")
    return app_id, app_secret


if __name__ == "__main__":
    APP_ID, APP_SECRET = _load_app_config()
    helper = WeChatDraftHelper(APP_ID, APP_SECRET)
    posts_directory = Path(__file__).resolve().parent / "posts"
    publish_posts_to_wechat(helper, posts_directory)
    